{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ray/anaconda3/lib/python3.10/site-packages/pinecone/index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "809036764b7f425cae5394a033ec8cc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "648d9af76b264f388a862229d6a5facd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73a06c6cc4204c96a41aec5b03b21664",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b2ce07ab4104c979ba66209e50e5520",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "# os.environ[\"PINECONE_API_KEY\"] = \"...\" # OR have this in your environment variables\n",
    "import getpass\n",
    "\n",
    "os.environ[\"PINECONE_API_KEY\"] = getpass.getpass(\"Pinecone API Key:\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Anyscale Endpoint API Key:\")\n",
    "\n",
    "from llama_index.vector_stores import PineconeVectorStore\n",
    "from llama_index.vector_stores.types import VectorStoreQuery, VectorStoreQueryMode\n",
    "\n",
    "vector_store = PineconeVectorStore(\n",
    "    index_name=\"quickstart\",\n",
    "    environment=\"us-west1-gcp-free\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "model_name=\"sentence-transformers/all-mpnet-base-v2\", \n",
    "\n",
    "def retrieve(query, limit=3750):\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "    query_embedding = embeddings.embed_query(query)\n",
    "\n",
    "    vectorStoreQuery = VectorStoreQuery(\n",
    "        query_embedding=query_embedding,\n",
    "        similarity_top_k=3,  \n",
    "        mode=VectorStoreQueryMode.DEFAULT,\n",
    "        query_str=None,  \n",
    "        alpha=None,\n",
    "        filters=None \n",
    "    )\n",
    "\n",
    "    res = vector_store.query(vectorStoreQuery)\n",
    "    contexts = []\n",
    "\n",
    "    for node in res.nodes[:]:\n",
    "        text = node.text\n",
    "        context = {'text': text}\n",
    "        contexts.append(context)\n",
    "\n",
    "    prompt_start = \"Answer the question based on the context below.\\n\\nContext:\\n\"\n",
    "    prompt_end = f\"\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "    prompt = prompt_start\n",
    "    for i in range(len(contexts)):\n",
    "        text_to_add = \"\\n\\n---\\n\\n\".join([context['text'] for context in contexts[:i+1]])\n",
    "        if len(prompt + text_to_add + prompt_end) >= limit:\n",
    "            break\n",
    "        prompt += text_to_add\n",
    "\n",
    "    prompt += prompt_end\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "question =\"\"\"'Does 'Ray Serve' support streaming?\"\"\"\n",
    "prompt = retrieve(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Answer the question based on the context below.\\n\\nContext:\\nAdvanced Guides#\\n\\nIf you’re new to Ray Serve, we recommend starting with the Ray Serve Quickstart.\\n\\nUse these advanced guides for more options and configurations:\\n\\nPass Arguments to Applications\\n\\nPerformance Tuning\\n\\nDynamic Request Batching\\n\\nIn-Place Updates for Serve\\n\\nDevelopment Workflow\\n\\nRay Serve Dashboard\\n\\nExperimental Java API\\n\\nMigrate from 1.x to 2.x\\n\\nExperimental gRPC SupportAdvanced Guides#\\n\\nIf you’re new to Ray Serve, we recommend starting with the Ray Serve Quickstart.\\n\\nUse these advanced guides for more options and configurations:\\n\\nPass Arguments to Applications\\n\\nPerformance Tuning\\n\\nDynamic Request Batching\\n\\nIn-Place Updates for Serve\\n\\nDevelopment Workflow\\n\\nRay Serve Dashboard\\n\\nExperimental Java API\\n\\nMigrate from 1.x to 2.x\\n\\nExperimental gRPC Support\\n\\n---\\n\\nAdvanced Guides#\\n\\nIf you’re new to Ray Serve, we recommend starting with the Ray Serve Quickstart.\\n\\nUse these advanced guides for more options and configurations:\\n\\nPass Arguments to Applications\\n\\nPerformance Tuning\\n\\nDynamic Request Batching\\n\\nIn-Place Updates for Serve\\n\\nDevelopment Workflow\\n\\nRay Serve Dashboard\\n\\nExperimental Java API\\n\\nMigrate from 1.x to 2.x\\n\\nExperimental gRPC SupportAdvanced Guides#\\n\\nIf you’re new to Ray Serve, we recommend starting with the Ray Serve Quickstart.\\n\\nUse these advanced guides for more options and configurations:\\n\\nPass Arguments to Applications\\n\\nPerformance Tuning\\n\\nDynamic Request Batching\\n\\nIn-Place Updates for Serve\\n\\nDevelopment Workflow\\n\\nRay Serve Dashboard\\n\\nExperimental Java API\\n\\nMigrate from 1.x to 2.x\\n\\nExperimental gRPC Support\\n\\n---\\n\\nAdvanced Guides#\\n\\nIf you’re new to Ray Serve, we recommend starting with the Ray Serve Quickstart.\\n\\nUse these advanced guides for more options and configurations:\\n\\nPass Arguments to Applications\\n\\nPerformance Tuning\\n\\nDynamic Request Batching\\n\\nIn-Place Updates for Serve\\n\\nDevelopment Workflow\\n\\nRay Serve Dashboard\\n\\nExperimental Java API\\n\\nMigrate from 1.x to 2.x\\n\\nExperimental gRPC Support\\n\\n---\\n\\nAdvanced Guides#\\n\\nIf you’re new to Ray Serve, we recommend starting with the Ray Serve Quickstart.\\n\\nUse these advanced guides for more options and configurations:\\n\\nPass Arguments to Applications\\n\\nPerformance Tuning\\n\\nDynamic Request Batching\\n\\nIn-Place Updates for Serve\\n\\nDevelopment Workflow\\n\\nRay Serve Dashboard\\n\\nExperimental Java API\\n\\nMigrate from 1.x to 2.x\\n\\nExperimental gRPC Support\\n\\nQuestion: 'Does 'Ray Serve' support streaming?\\nAnswer:\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'meta-llama/Llama-2-70b-chat-hf-86e450c1-53fd-4b1a-acc3-b83046823873', 'object': 'text_completion', 'created': 1692031409, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'message': {'role': 'assistant', 'content': 'Yes, Ray Serve supports streaming. One of the advanced guides listed is \"Experimental gRPC Support,\" which suggests that Ray Serve may have the ability to stream data using gRPC. Additionally, the guide on \"Dynamic Request Batching\" may also be relevant for optimizing streaming requests.'}, 'index': 0, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 776, 'completion_tokens': 65, 'total_tokens': 841}}\n"
     ]
    }
   ],
   "source": [
    "#Example using Python\n",
    "import os\n",
    "import requests\n",
    "\n",
    "s = requests.Session()\n",
    "\n",
    "#api_base = os.getenv(\"OPENAI_API_BASE\")\n",
    "api_base = \"https://api.endpoints.anyscale.com/v1\"\n",
    "token = os.getenv(\"OPENAI_API_KEY\")\n",
    "url = f\"{api_base}/chat/completions\"\n",
    "body = {\n",
    "  \"model\": \"meta-llama/Llama-2-70b-chat-hf\",\n",
    "  \"messages\": [{\"role\": \"system\", \"content\": prompt}, {\"role\": \"user\", \"content\": question}],\n",
    "  \"temperature\": 0.7\n",
    "}\n",
    "\n",
    "with s.post(url, headers={\"Authorization\": f\"Bearer {token}\"}, json=body) as resp:\n",
    "    print(resp.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"meta-llama/Llama-2-70b-chat-hf-15d6298f-a159-4e7f-9861-1f7b37e91111\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1692030385,\n",
      "  \"model\": \"meta-llama/Llama-2-70b-chat-hf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"Yes, Ray Serve supports streaming. You can use the Experimental gRPC Support advanced guide to configure streaming for your applications.\"\n",
      "      },\n",
      "      \"index\": 0,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 776,\n",
      "    \"completion_tokens\": 28,\n",
      "    \"total_tokens\": 804\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#Examples using OpenAI SDK\n",
    "import openai\n",
    "\n",
    "# Make sure you have set the correct env vars\n",
    "# You can also set the openai environment manually as shown\n",
    "openai.api_base = \"https://api.endpoints.anyscale.com/v1\"\n",
    "\n",
    "\n",
    "# Note: not all arguments are currently supported and will be ignored by the backend.\n",
    "chat_completion = openai.ChatCompletion.create(\n",
    "    model=\"meta-llama/Llama-2-70b-chat-hf\",\n",
    "    messages=[{\"role\": \"system\", \"content\": prompt}, {\"role\": \"user\", \"content\": question}],\n",
    "    temperature=0.01\n",
    ")\n",
    "print(chat_completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamaindex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
